#!/usr/bin/env python3
"""
Optimized Enhanced Offset Motion Tracker
=======================================

A carefully optimized enhanced offset tracker that stays within 500K parameters
while providing significant improvements over the baseline model.

Parameter Budget Allocation:
- Motion Encoder: ~150K parameters (optimized multi-scale)
- Object Interaction: ~200K parameters (efficient attention)
- Offset Predictor: ~150K parameters (enhanced prediction)
- Total Target: <500K parameters

Key Features:
1. Lightweight multi-scale motion encoding
2. Efficient self-attention for object interactions  
3. Confidence-aware offset prediction
4. Optimized parameter usage
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
from torchvision.ops import roi_align
import math


class LightweightMotionEncoder(nn.Module):
    """Lightweight multi-scale motion encoder - Budget: ~150K parameters"""
    
    def __init__(self, input_channels=2, base_channels=32):
        super().__init__()
        
        # Initial encoding
        self.conv1 = nn.Sequential(
            nn.Conv2d(input_channels, base_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(base_channels),
            nn.ReLU(inplace=True),
        )
        
        # Multi-scale branches
        self.branch1 = nn.Sequential(
            nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(base_channels),
            nn.ReLU(inplace=True),
        )
        
        self.branch2 = nn.Sequential(
            nn.Conv2d(base_channels, base_channels, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(base_channels),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(base_channels, base_channels, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm2d(base_channels),
            nn.ReLU(inplace=True),
        )
        
        # Feature fusion
        self.fusion = nn.Sequential(
            nn.Conv2d(base_channels * 2, base_channels * 2, kernel_size=3, padding=1),
            nn.BatchNorm2d(base_channels * 2),
            nn.ReLU(inplace=True),
        )
        
        # Channel attention (lightweight)
        self.attention = LightweightChannelAttention(base_channels * 2)
        
        # Output projection
        self.output_proj = nn.Sequential(
            nn.Conv2d(base_channels * 2, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
        )
    
    def forward(self, mv_map):
        """
        Args:
            mv_map: [B, 2, H, W] motion vector map
        Returns:
            features: [B, 64, H, W] encoded features
        """
        x = self.conv1(mv_map)     # [B, 32, H, W]
        
        # Multi-scale processing
        b1 = self.branch1(x)       # [B, 32, H, W]
        b2 = self.branch2(x)       # [B, 32, H, W]
        
        # Fusion
        fused = torch.cat([b1, b2], dim=1)  # [B, 64, H, W]
        fused = self.fusion(fused)          # [B, 64, H, W]
        
        # Attention and output
        attended = self.attention(fused)
        output = self.output_proj(attended)  # [B, 64, H, W]
        
        return output


class LightweightChannelAttention(nn.Module):
    """Lightweight channel attention - minimal parameters"""
    
    def __init__(self, channels, reduction=4):
        super().__init__()
        
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        B, C, H, W = x.size()
        y = self.avg_pool(x).view(B, C)
        y = self.fc(y).view(B, C, 1, 1)
        return x * y


class EfficientObjectInteraction(nn.Module):
    """Efficient object interaction module - Budget: ~200K parameters"""
    
    def __init__(self, feature_dim=128, n_heads=4, hidden_dim=256):
        super().__init__()
        
        self.feature_dim = feature_dim
        self.n_heads = n_heads
        self.head_dim = feature_dim // n_heads
        
        # Position encoding
        self.pos_encoder = nn.Sequential(
            nn.Linear(2, 32),
            nn.ReLU(inplace=True),
            nn.Linear(32, feature_dim)
        )
        
        # Self-attention (shared QKV projection for efficiency)
        self.qkv_proj = nn.Linear(feature_dim, feature_dim * 3)
        self.out_proj = nn.Linear(feature_dim, feature_dim)
        
        # Layer norm and feedforward
        self.norm1 = nn.LayerNorm(feature_dim)
        self.norm2 = nn.LayerNorm(feature_dim)
        
        self.ffn = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, feature_dim)
        )
        
        self.scale = self.head_dim ** -0.5
    
    def forward(self, object_features, positions):
        """Apply efficient object interaction"""
        N = object_features.size(0)
        
        if N <= 1:
            return object_features
        
        # Position encoding
        norm_positions = positions / 320.0  # Normalize to [-1, 1] roughly
        pos_encoding = self.pos_encoder(norm_positions)
        x = object_features + pos_encoding
        
        # Self-attention
        residual = x
        x = self.norm1(x)
        
        # Compute Q, K, V
        qkv = self.qkv_proj(x).reshape(N, 3, self.n_heads, self.head_dim)
        q, k, v = qkv.unbind(1)  # Each: [N, n_heads, head_dim]
        
        # Attention scores
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # [N, n_heads, N]
        attn_weights = F.softmax(scores, dim=-1)
        
        # Apply attention
        attn_output = torch.matmul(attn_weights, v)  # [N, n_heads, head_dim]
        attn_output = attn_output.reshape(N, self.feature_dim)
        attn_output = self.out_proj(attn_output)
        
        # First residual
        x = residual + attn_output
        
        # Feedforward with second residual
        residual = x
        x = self.norm2(x)
        x = residual + self.ffn(x)
        
        return x


class OptimizedOffsetPredictor(nn.Module):
    """Optimized offset predictor - Budget: ~150K parameters"""
    
    def __init__(self, 
                 mv_feature_dim=64,
                 object_feature_dim=128,
                 roi_size=8,  # Smaller ROI for efficiency
                 hidden_dim=256):
        super().__init__()
        
        self.roi_size = roi_size
        self.mv_feature_dim = mv_feature_dim
        self.object_feature_dim = object_feature_dim
        
        # Efficient ROI processing
        self.roi_conv = nn.Sequential(
            nn.Conv2d(mv_feature_dim, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((3, 3)),  # Smaller output
            nn.Flatten(),
            nn.Linear(32 * 9, 64)
        )
        
        # Motion statistics
        self.mv_stats_extractor = nn.Sequential(
            nn.Linear(6, 32),  # Reduced statistics
            nn.ReLU(inplace=True),
            nn.Linear(32, 48),
        )
        
        # Geometry encoding
        self.geometry_encoder = nn.Sequential(
            nn.Linear(4, 24),  # Basic geometry
            nn.ReLU(inplace=True),
            nn.Linear(24, 32),
        )
        
        # Feature fusion
        feature_input_dim = 64 + 48 + 32  # roi + mv_stats + geometry
        self.feature_fusion = nn.Sequential(
            nn.Linear(feature_input_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, object_feature_dim)
        )
        
        # Offset prediction heads
        self.offset_head = nn.Sequential(
            nn.Linear(object_feature_dim, 96),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(96, 4)
        )
        
        # Confidence head
        self.confidence_head = nn.Sequential(
            nn.Linear(object_feature_dim, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, 4),
            nn.Sigmoid()
        )
        
        # Initialize heads for better offset prediction
        nn.init.normal_(self.offset_head[-1].weight, mean=0.0, std=0.2)  # Larger std for bigger offsets
        nn.init.zeros_(self.offset_head[-1].bias)
        
        # Initialize confidence head to be more optimistic initially
        nn.init.normal_(self.confidence_head[-2].weight, mean=0.0, std=0.1)
        nn.init.constant_(self.confidence_head[-2].bias, 2.0)  # Higher initial confidence
    
    def extract_features(self, mv_features, mv_map, boxes):
        """Extract optimized features for each bounding box"""
        if len(boxes) == 0:
            return torch.empty(0, self.object_feature_dim, device=boxes.device)
        
        # Convert to pixel coordinates
        H, W = mv_map.shape[-2:]
        cx = boxes[:, 0] * W
        cy = boxes[:, 1] * H
        w = boxes[:, 2] * W
        h = boxes[:, 3] * H
        
        x1 = cx - w / 2
        y1 = cy - h / 2
        x2 = cx + w / 2
        y2 = cy + h / 2
        
        # ROI boxes for alignment
        batch_indices = torch.zeros(len(boxes), 1, device=boxes.device)
        roi_boxes = torch.cat([batch_indices, x1.unsqueeze(1), y1.unsqueeze(1), 
                              x2.unsqueeze(1), y2.unsqueeze(1)], dim=1)
        
        # Extract ROI features
        roi_features = roi_align(
            mv_features, roi_boxes,
            output_size=(self.roi_size, self.roi_size),
            spatial_scale=1.0,
            aligned=True
        )
        
        mv_roi = roi_align(
            mv_map, roi_boxes,
            output_size=(self.roi_size, self.roi_size),
            spatial_scale=1.0,
            aligned=True
        )
        
        # Process ROI features
        roi_processed = self.roi_conv(roi_features)
        
        # Motion vector statistics (simplified)
        mv_u = mv_roi[:, 0].flatten(1)
        mv_v = mv_roi[:, 1].flatten(1)
        
        mean_u = mv_u.mean(dim=1)
        mean_v = mv_v.mean(dim=1)
        std_u = mv_u.std(dim=1)
        std_v = mv_v.std(dim=1)
        magnitude = torch.sqrt(mean_u**2 + mean_v**2)
        direction = torch.atan2(mean_v, mean_u)
        
        mv_stats = torch.stack([mean_u, mean_v, std_u, std_v, magnitude, direction], dim=1)
        mv_processed = self.mv_stats_extractor(mv_stats)
        
        # Basic geometry features
        aspect_ratio = w / (h + 1e-8)
        area = w * h / (W * H)  # Normalized area
        
        geometry = torch.stack([w/W, h/H, aspect_ratio, area], dim=1)
        geo_processed = self.geometry_encoder(geometry)
        
        # Feature fusion
        combined = torch.cat([roi_processed, mv_processed, geo_processed], dim=1)
        object_features = self.feature_fusion(combined)
        
        return object_features
    
    def forward(self, mv_features, mv_map, boxes, enhanced_features=None):
        """
        Args:
            mv_features: [1, 64, H, W] encoded motion features
            mv_map: [1, 2, H, W] raw motion vectors
            boxes: [N, 4] YOLO boxes
            enhanced_features: [N, feature_dim] enhanced features (optional)
        Returns:
            offsets: [N, 4] predicted offsets
            confidences: [N, 4] prediction confidences
        """
        if len(boxes) == 0:
            return torch.empty(0, 4, device=boxes.device), torch.empty(0, 4, device=boxes.device)
        
        # Use enhanced features if available, otherwise extract
        if enhanced_features is not None:
            object_features = enhanced_features
        else:
            object_features = self.extract_features(mv_features, mv_map, boxes)
        
        # Predict offsets and confidences
        offsets = self.offset_head(object_features)
        confidences = self.confidence_head(object_features)
        
        return offsets, confidences


class OptimizedEnhancedOffsetTracker(nn.Module):
    """Optimized enhanced offset tracker - Target: <500K parameters"""
    
    def __init__(self,
                 mv_channels=2,
                 base_channels=32,
                 object_feature_dim=128,
                 roi_size=8,
                 hidden_dim=256,
                 n_attention_heads=4,
                 image_size=(640, 640)):
        super().__init__()
        
        self.image_size = image_size
        self.object_feature_dim = object_feature_dim
        
        # Optimized components
        self.mv_encoder = LightweightMotionEncoder(mv_channels, base_channels)
        self.object_interaction = EfficientObjectInteraction(object_feature_dim, n_attention_heads, hidden_dim)
        self.offset_predictor = OptimizedOffsetPredictor(
            mv_feature_dim=64,
            object_feature_dim=object_feature_dim,
            roi_size=roi_size,
            hidden_dim=hidden_dim
        )
    
    def apply_offsets_with_confidence(self, boxes, offsets, confidences):
        """Apply predicted offsets with confidence weighting in normalized coordinate space"""
        if len(boxes) == 0:
            return boxes, torch.empty(0, device=boxes.device)
        
        # Apply confidence-weighted offsets directly in normalized space
        weighted_offsets = offsets * confidences
        
        new_cx = boxes[:, 0] + weighted_offsets[:, 0]
        new_cy = boxes[:, 1] + weighted_offsets[:, 1]
        new_w = boxes[:, 2] + weighted_offsets[:, 2]
        new_h = boxes[:, 3] + weighted_offsets[:, 3]
        
        # Clamp to valid normalized ranges [0, 1]
        new_cx = torch.clamp(new_cx, new_w/2, 1.0 - new_w/2)
        new_cy = torch.clamp(new_cy, new_h/2, 1.0 - new_h/2)
        new_w = torch.clamp(new_w, 0.01, 0.9)  # Reasonable size limits
        new_h = torch.clamp(new_h, 0.01, 0.9)
        
        new_boxes = torch.stack([new_cx, new_cy, new_w, new_h], dim=1)
        confidence_scores = torch.mean(confidences, dim=1)
        
        return new_boxes, confidence_scores
    
    def forward(self, mv_map, prev_boxes):
        """
        Args:
            mv_map: [1, 2, H, W] motion vector field
            prev_boxes: [N, 4] previous bounding boxes (YOLO format)
        Returns:
            dict with predictions
        """
        if len(prev_boxes) == 0:
            return {
                'predicted_offsets': torch.empty(0, 4),
                'predicted_confidences': torch.empty(0, 4),
                'updated_boxes': torch.empty(0, 4),
                'confidence_scores': torch.empty(0),
                'mv_features': None
            }
        
        # Encode motion vectors
        mv_features = self.mv_encoder(mv_map)  # [1, 64, H, W]
        
        # Extract initial object features
        initial_features = self.offset_predictor.extract_features(mv_features, mv_map, prev_boxes)
        
        # Apply object interactions if multiple objects
        if len(prev_boxes) > 1:
            H, W = self.image_size
            positions = torch.stack([
                prev_boxes[:, 0] * W,
                prev_boxes[:, 1] * H
            ], dim=1)
            
            enhanced_features = self.object_interaction(initial_features, positions)
        else:
            enhanced_features = initial_features
        
        # Predict offsets and confidences
        predicted_offsets, predicted_confidences = self.offset_predictor(
            mv_features, mv_map, prev_boxes, enhanced_features
        )
        
        # Apply offsets with confidence weighting
        updated_boxes, confidence_scores = self.apply_offsets_with_confidence(
            prev_boxes, predicted_offsets, predicted_confidences
        )
        
        return {
            'predicted_offsets': predicted_offsets,
            'predicted_confidences': predicted_confidences,
            'updated_boxes': updated_boxes,
            'confidence_scores': confidence_scores,
            'mv_features': mv_features,
            'enhanced_features': enhanced_features
        }


class OptimizedEnhancedLoss(nn.Module):
    """Optimized enhanced loss function"""
    
    def __init__(self, 
                 offset_weight=5.0,  # Increased from 2.0 to emphasize offset learning
                 iou_weight=1.0,
                 confidence_weight=0.1):
        super().__init__()
        
        self.offset_weight = offset_weight
        self.iou_weight = iou_weight
        self.confidence_weight = confidence_weight
    
    def compute_target_offsets(self, prev_boxes, target_boxes, image_size=(640, 640)):
        """Compute ground truth offsets in normalized coordinate space (matching model predictions)"""
        # Keep everything in normalized coordinates to match model output scale
        offset_cx = target_boxes[:, 0] - prev_boxes[:, 0]  # Normalized cx offset
        offset_cy = target_boxes[:, 1] - prev_boxes[:, 1]  # Normalized cy offset 
        offset_w = target_boxes[:, 2] - prev_boxes[:, 2]   # Normalized w offset
        offset_h = target_boxes[:, 3] - prev_boxes[:, 3]   # Normalized h offset
        
        target_offsets = torch.stack([offset_cx, offset_cy, offset_w, offset_h], dim=1)
        return target_offsets
    
    def compute_iou_loss(self, pred_boxes, target_boxes):
        """Compute IoU loss"""
        pred_x1 = pred_boxes[:, 0] - pred_boxes[:, 2] / 2
        pred_y1 = pred_boxes[:, 1] - pred_boxes[:, 3] / 2
        pred_x2 = pred_boxes[:, 0] + pred_boxes[:, 2] / 2
        pred_y2 = pred_boxes[:, 1] + pred_boxes[:, 3] / 2
        
        target_x1 = target_boxes[:, 0] - target_boxes[:, 2] / 2
        target_y1 = target_boxes[:, 1] - target_boxes[:, 3] / 2
        target_x2 = target_boxes[:, 0] + target_boxes[:, 2] / 2
        target_y2 = target_boxes[:, 1] + target_boxes[:, 3] / 2
        
        inter_x1 = torch.max(pred_x1, target_x1)
        inter_y1 = torch.max(pred_y1, target_y1)
        inter_x2 = torch.min(pred_x2, target_x2)
        inter_y2 = torch.min(pred_y2, target_y2)
        
        inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)
        pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)
        target_area = (target_x2 - target_x1) * (target_y2 - target_y1)
        union_area = pred_area + target_area - inter_area + 1e-8
        
        iou = inter_area / union_area
        iou_loss = 1 - iou
        
        return iou_loss.mean(), iou.mean(), iou  # Return individual IoUs as well
    
    def forward(self, predictions, targets):
        """Compute enhanced loss"""
        pred_offsets = predictions['predicted_offsets']
        pred_confidences = predictions['predicted_confidences']
        updated_boxes = predictions['updated_boxes']
        
        prev_boxes = targets['prev_boxes']
        target_boxes = targets['target_boxes']
        
        # Ground truth offsets
        target_offsets = self.compute_target_offsets(prev_boxes, target_boxes)
        
        # Confidence-weighted offset loss
        offset_diff = torch.abs(pred_offsets - target_offsets)
        weighted_offset_loss = (offset_diff * pred_confidences).mean()
        
        # IoU loss
        iou_loss, mean_iou, individual_ious = self.compute_iou_loss(updated_boxes, target_boxes)
        
        # Confidence regularization
        confidence_reg = F.mse_loss(pred_confidences.mean(dim=1), 
                                   torch.clamp(mean_iou.detach().expand(len(pred_confidences)), 0.2, 0.9))
        
        # Total loss
        total_loss = (self.offset_weight * weighted_offset_loss + 
                     self.iou_weight * iou_loss + 
                     self.confidence_weight * confidence_reg)
        
        loss_components = {
            'total_loss': total_loss.item(),
            'offset_loss': weighted_offset_loss.item(),
            'iou_loss': iou_loss.item(),
            'confidence_reg': confidence_reg.item(),
            'mean_iou': mean_iou.item(),
            'individual_ious': individual_ious  # Include individual IoU values
        }
        
        return total_loss, loss_components


if __name__ == "__main__":
    print("üöÄ Testing Optimized Enhanced Offset Motion Tracker")
    print("=" * 60)
    
    # Create optimized model
    model = OptimizedEnhancedOffsetTracker()
    criterion = OptimizedEnhancedLoss()
    
    # Count parameters by component
    encoder_params = sum(p.numel() for p in model.mv_encoder.parameters())
    interaction_params = sum(p.numel() for p in model.object_interaction.parameters())
    predictor_params = sum(p.numel() for p in model.offset_predictor.parameters())
    total_params = sum(p.numel() for p in model.parameters())
    
    print("üìä OPTIMIZED MODEL PARAMETER BREAKDOWN:")
    print(f"   üîß Lightweight Motion Encoder: {encoder_params:,} parameters")
    print(f"   ü§ù Efficient Object Interaction: {interaction_params:,} parameters")
    print(f"   üéØ Optimized Offset Predictor: {predictor_params:,} parameters")
    print(f"   üìà TOTAL: {total_params:,} parameters")
    print()
    
    if total_params <= 500000:
        print(f"‚úÖ WITHIN LIMIT: {total_params:,} / 500,000 parameters")
        print(f"   üìä Usage: {total_params/500000*100:.1f}% of allowed parameters")
        efficiency = total_params / 24700  # vs original model
        print(f"   üìà Enhancement: {efficiency:.1f}x more parameters than original")
        remaining = 500000 - total_params
        print(f"   üéØ Remaining budget: {remaining:,} parameters")
    else:
        print(f"‚ùå EXCEEDS LIMIT: {total_params:,} / 500,000 parameters")
        print(f"   üìä Overage: {total_params-500000:,} parameters")
    
    print()
    print("üß™ TESTING OPTIMIZED ENHANCED CAPABILITIES:")
    
    # Test with realistic scenario
    mv_map = torch.randn(1, 2, 40, 40)
    prev_boxes = torch.rand(8, 4)  # Test with 8 objects
    target_boxes = prev_boxes + torch.randn(8, 4) * 0.02
    
    print(f"   Input: {prev_boxes.shape[0]} objects")
    print(f"   Motion vectors: {mv_map.shape}")
    
    # Forward pass
    try:
        with torch.no_grad():
            predictions = model(mv_map, prev_boxes)
        
        print(f"   ‚úÖ Forward pass successful!")
        print(f"   Predicted offsets: {predictions['predicted_offsets'].shape}")
        print(f"   Predicted confidences: {predictions['predicted_confidences'].shape}")
        print(f"   Updated boxes: {predictions['updated_boxes'].shape}")
        print(f"   Confidence scores: {predictions['confidence_scores'].shape}")
        
        # Sample outputs
        sample_offsets = predictions['predicted_offsets'][:3]
        sample_confidences = predictions['predicted_confidences'][:3]
        sample_scores = predictions['confidence_scores'][:3]
        
        print(f"   Sample offsets: {sample_offsets}")
        print(f"   Sample confidences: {sample_confidences}")
        print(f"   Sample confidence scores: {sample_scores}")
        
        # Test loss computation
        targets = {
            'prev_boxes': prev_boxes,
            'target_boxes': target_boxes
        }
        
        loss, components = criterion(predictions, targets)
        print()
        print("üî• OPTIMIZED ENHANCED LOSS ANALYSIS:")
        print(f"   Total loss: {loss.item():.4f}")
        print(f"   Offset loss: {components['offset_loss']:.4f}")
        print(f"   IoU loss: {components['iou_loss']:.4f}")
        print(f"   Confidence reg: {components['confidence_reg']:.4f}")
        print(f"   Mean IoU: {components['mean_iou']:.4f}")
        
        # Test backward pass
        loss.backward()
        print("   ‚úÖ Backward pass successful!")
        
        print()
        print("üéØ OPTIMIZED ENHANCED MODEL SUMMARY:")
        print(f"   ‚úÖ {total_params:,} parameters (within 500K limit!)")
        print(f"   ‚úÖ {efficiency:.1f}x more capacity than original")
        print(f"   ‚úÖ Lightweight multi-scale motion encoding")
        print(f"   ‚úÖ Efficient object interaction with self-attention")
        print(f"   ‚úÖ Confidence-aware offset prediction")
        print(f"   ‚úÖ Optimized parameter allocation")
        print(f"   ‚úÖ Enhanced geometric and motion feature extraction")
        print(f"   ‚úÖ Ready for significant performance improvement!")
        print(f"   üöÄ Expected substantial gains over 0.1039 IoU baseline!")
        
    except Exception as e:
        print(f"   ‚ùå Error during testing: {e}")
        import traceback
        traceback.print_exc()
